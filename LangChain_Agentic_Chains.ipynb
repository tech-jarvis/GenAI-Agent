{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fahad-Aslam/LangChain/blob/main/LangChain_Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEJ8ofx2ZTI9",
        "outputId": "36a11480-d8c6-465f-cb21-74f7df778243"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoznKBIsZVjl"
      },
      "outputs": [],
      "source": [
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY34c0nSZfK1",
        "outputId": "88a80f3e-a8b0-41ed-bed7-5388e467fd5d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "documents = TextLoader('/content/drive/MyDrive/docs.txt').load()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsAC5vt0bts2"
      },
      "source": [
        "# Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X501NVEcbvoH"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH7aNH9EbviB",
        "outputId": "fc2007a0-eeb2-4df6-8ce5-50f21930c771"
      },
      "outputs": [],
      "source": [
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeOAeRRKbvb8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEzgFyd-bvV-"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC40sZNsb3cJ"
      },
      "outputs": [],
      "source": [
        "#repo_id = \"google/flan-t5-xxl\"\n",
        "repo_id = \"tiiuae/falcon-7b-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFdOOcCUb3XM"
      },
      "outputs": [],
      "source": [
        "llm= HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5,\n",
        "                                   \"max_new_tokens\":400,\n",
        "                                   \"max_length\": 300,\n",
        "                                   \"num_return_sequences\":300,\n",
        "                                   \"batch_size\":5\n",
        "                                  #  \"top_k\": 50,\n",
        "                                  #  \"top_p\": .95,\n",
        "                                  #  \"do_sample\": True,\n",
        "                                  #  \"early_stopping\": False,\n",
        "                                  #  \"num_beams\": 1,\n",
        "                                  #  \"no_repeat_ngram_size\": 3\n",
        "                                   }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NVaImkbb3SD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upVBgotgZr4y"
      },
      "source": [
        "# Embedding the document and chunking it into 1000 words/chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwOB7PpdZiYu"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install chromadb==0.3.29\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2tITKv2Z5Iy"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain import VectorDBQA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PuJU214Z7-b"
      },
      "outputs": [],
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "#embeddings = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# embeddings = SentenceTransformerEmbeddings(model_name=\"tiiuae/falcon-7b-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBbI1FR9Z97x",
        "outputId": "6b5afd0d-31ed-4a3c-9624-f4c77dfa3617"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPFTEleGbKhy"
      },
      "outputs": [],
      "source": [
        "# retriever = FAISS.from_documents(texts, llm).as_retriever()\n",
        "vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKbFKQRFaaRc"
      },
      "outputs": [],
      "source": [
        "query = \"what is finma\"\n",
        "docs = retriever.get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoKBSbt9bY2p",
        "outputId": "a55cb44d-2af1-42da-83d8-0f5c1a988d7c"
      },
      "outputs": [],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox0srNV3bfPy"
      },
      "source": [
        "# Loading QA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcxSVrXDa0YW"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S2EK-Erbi7w"
      },
      "source": [
        "# 1 Quick Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jgt2VljLbhnA",
        "outputId": "1dd35d54-da66-412d-8f9f-1bfa5b447d6e"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "query = \"what is finma\"\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLJ3fS06cp4-"
      },
      "source": [
        "# 2 The stuff Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKrZqAxAboeC"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWnGwxMRc1FN",
        "outputId": "adffa86d-a8e3-4082-9086-85bbdb3de5b0"
      },
      "outputs": [],
      "source": [
        "query = \"how does cobra ensures liquidity for banks\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6KBwAjHddwP"
      },
      "source": [
        "### Custom Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvHLURd-c9oW",
        "outputId": "11748e16-7874-4134-91c9-77899c641752"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in URDU:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSkvt19hd5VQ"
      },
      "source": [
        "# 3 The map_reduce Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IL7egP6dmvu"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"map_reduce\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qtw1Ayyd-TZ",
        "outputId": "8ed8115b-2f94-48d6-81c6-38cc6843a8ea"
      },
      "outputs": [],
      "source": [
        "query = \"What is finma?\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtNEaB7IeWJY"
      },
      "source": [
        "### Intermediate Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmtcoXg_d_sP"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"map_reduce\", return_map_steps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aveo776UebiK",
        "outputId": "0de8a5a9-8c92-40f6-f9b3-be1e635e99a8"
      },
      "outputs": [],
      "source": [
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN_dHdpcepZH"
      },
      "source": [
        "### Custom Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INYhWuLbecs1",
        "outputId": "c3681aae-1614-471a-f543-7d58b836bc1a"
      },
      "outputs": [],
      "source": [
        "question_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
        "Return any relevant text translated into italian.\n",
        "{context}\n",
        "Question: {question}\n",
        "Relevant text, if any, in Italian:\"\"\"\n",
        "QUESTION_PROMPT = PromptTemplate(\n",
        "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "combine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer italian.\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "\n",
        "QUESTION: {question}\n",
        "=========\n",
        "{summaries}\n",
        "=========\n",
        "Answer in Italian:\"\"\"\n",
        "COMBINE_PROMPT = PromptTemplate(\n",
        "    template=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\n",
        ")\n",
        "chain = load_qa_chain(llm, chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaoRa-zBfaG8"
      },
      "source": [
        "# 4 The refine Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Xq7IeAe17M"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"refine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZpR535AfhHd",
        "outputId": "515a45f3-ffdd-4cfd-aed9-0207552df40f"
      },
      "outputs": [],
      "source": [
        "query = \"What is finma in swiss\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-H5XdAwftWR"
      },
      "source": [
        "### Intermediate Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpPZiDtUfjxN"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"refine\", return_refine_steps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXTfq9u9fwSd",
        "outputId": "f33889d0-fc29-42ca-9b62-ab8cc4ab7e81"
      },
      "outputs": [],
      "source": [
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEKPzE8gf8Vr"
      },
      "source": [
        "### Custom Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGDP8qjffywd",
        "outputId": "1077dbd3-652a-46c5-8c47-2f9562c9a053"
      },
      "outputs": [],
      "source": [
        "refine_prompt_template = (\n",
        "    \"The original question is as follows: {question}\\n\"\n",
        "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question. \"\n",
        "    \"If the context isn't useful, return the original answer. Reply in Italian.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
        "    template=refine_prompt_template,\n",
        ")\n",
        "\n",
        "\n",
        "initial_qa_template = (\n",
        "    \"Context information is below. \\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question: {question}\\nYour answer should be in Italian.\\n\"\n",
        ")\n",
        "initial_qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context_str\", \"question\"], template=initial_qa_template\n",
        ")\n",
        "chain = load_qa_chain(llm, chain_type=\"refine\", return_refine_steps=True,\n",
        "                     question_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mJhMwtmgPmV"
      },
      "source": [
        "# 5 The map-rerank Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENVMTGQqgHHz"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"map_rerank\", return_intermediate_steps=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-_0UK6EgSnE"
      },
      "outputs": [],
      "source": [
        "query = \"what is not finma\"\n",
        "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rz3qzMLpgTu7",
        "outputId": "077901db-643b-45dc-c417-7942f959b94d"
      },
      "outputs": [],
      "source": [
        "results[\"output_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuyZceI_hAED"
      },
      "source": [
        "### Intermediate steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-8q4uctgWE3",
        "outputId": "3150f55b-8251-43c3-81ab-996974e58a39"
      },
      "outputs": [],
      "source": [
        "results[\"intermediate_steps\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rQn8z1ShCPX"
      },
      "source": [
        "### Custom Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "nD5H1GUAgc0e",
        "outputId": "42716e15-6c00-4f79-9c0b-02d4afc7d12a"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "output_parser = RegexParser(\n",
        "    regex=r\"(.*?)\\nScore: (.*)\",\n",
        "    output_keys=[\"answer\", \"score\"],\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
        "\n",
        "Question: [question here]\n",
        "Helpful Answer In Italian: [answer here]\n",
        "Score: [score between 0 and 100]/\n",
        "\n",
        "\n",
        "Begin!\n",
        "\n",
        "Context:\n",
        "---------\n",
        "{context}\n",
        "---------\n",
        "Question: {question}\n",
        "Helpful Answer In Italian:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    output_parser=output_parser,\n",
        "    # handle_parsing_errors=True\n",
        "\n",
        ")\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT)\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es4ep_Sqit8l"
      },
      "source": [
        "#6 Document QA with sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx7tGWVyj33p",
        "outputId": "efbd5659-8d7b-457b-c9bc-4e21f697baf4"
      },
      "outputs": [],
      "source": [
        "# retriever = FAISS.from_documents(texts, llm).as_retriever()\n",
        "docsearch = Chroma.from_documents(documents=texts, embedding=embeddings)\n",
        "query = \"What is a candy\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEjrT0SlhEyT",
        "outputId": "407197fb-ecda-47a7-ebf0-34cf32ab1148"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "\n",
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
        "query = \"What is a cyclone\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4GIyrPgjAtD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN0vduhWSfDNorfiSw3B9mL",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
